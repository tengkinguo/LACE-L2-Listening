# -*- coding: utf-8 -*-
"""
Created on Thu Nov 27 16:48:57 2025

@author: T
"""

# -*- coding: utf-8 -*-
"""
Script: 4_generate_figure4_shap.py
Description:
    Generates Figure 4 (SHAP Beeswarm Plot) for the LACE model.
    It uses KernelSHAP to explain the contribution of:
      - Lexical features (Frequency, Length)
      - Phonological features (Syllables, Complexity)
      - Behavioral state (Previous Error)
    
    The Transformer history context is pre-computed and fixed for each sample
    to isolate the specific impact of the current-item features.

Usage:
    Ensure 'data/processed/test.npz', 'data/encoders.pkl', and
    'results/models/E1_LACE_run0.pt' exist before running.
"""

import os
import sys
import pickle
import numpy as np
import torch
import shap
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader
from tqdm import tqdm

# Add project root to sys.path to allow importing local modules
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(BASE_DIR)

# Try importing the model architecture
try:
    from scripts.model_arch import LACEModel
    from scripts.train_lace import UnifiedSAKTDataset, HPARAMS, load_vocab_sizes
except ImportError:
    # Fallback if running directly inside the scripts folder without package structure
    from model_arch import LACEModel
    from train_lace import UnifiedSAKTDataset, HPARAMS, load_vocab_sizes

# --- Configuration ---
MODEL_NAME = "E1_LACE"         # Must match the config used in training
RUN_ID = 0                     # Which run to analyze
N_SAMPLES = 500                # Number of test samples to use (SHAP is slow, 500 is sufficient for visualization)
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Paths
DATA_DIR = os.path.join(BASE_DIR, "data", "processed")
MODEL_PATH = os.path.join(BASE_DIR, "results", "models", f"{MODEL_NAME}_run{RUN_ID}.pt")
OUTPUT_FIG_PATH = os.path.join(BASE_DIR,  "figures", "Figure4_SHAP.png")

# Readable Feature Names for the Plot
FEATURE_NAMES = [
    "Log Frequency",       # next_lexical[0]
    "Word Length",         # next_lexical[1]
    "Syllable Count",      # next_phoneme[0]
    "Phonological Complexity", # next_phoneme[1] (is_challenging)
    "Previous Error"       # next_prev_interaction[1] (label)
]

class SHAPModelWrapper:
    """
    Wrapper to make the PyTorch model compatible with SHAP KernelExplainer.
    
    It takes a simplified input matrix (N, 5) representing the 5 features of interest,
    and reconstructs the full model input using fixed context vectors (h_last, prev_token).
    """
    def __init__(self, model, fixed_h_last, fixed_prev_token_id):
        self.model = model
        # Context vectors are specific to the sample being explained
        self.fixed_h_last = fixed_h_last            # Shape: (1, d_model)
        self.fixed_prev_token_id = fixed_prev_token_id  # Shape: (1,)
        self.model.eval()

    def predict(self, input_matrix):
        """
        Args:
            input_matrix (np.ndarray): Shape (N, 5).
                Cols: [Freq, Len, Syl, Complex, Prev_Label]
        Returns:
            np.ndarray: Model predictions (logits) of shape (N,)
        """
        # Convert numpy input to tensor
        inputs = torch.tensor(input_matrix, dtype=torch.float32, device=DEVICE)
        batch_size = inputs.shape[0]

        # 1. Reconstruct Lexical Features (Cols 0-1)
        lex_in = inputs[:, 0:2]
        lex_feat = self.model.lexical_mlp(lex_in)

        # 2. Reconstruct Phonological Features (Cols 2-3)
        pho_in = inputs[:, 2:4]
        pho_feat = self.model.phoneme_mlp(pho_in)

        # 3. Reconstruct Previous Interaction Features (Col 4)
        # Prev Label: Input is 0 (Correct) or 1 (Wrong).
        # Embedding expects index + 1 (since 0 is padding).
        # We round to int because SHAP passes floats.
        prev_label_idx = (inputs[:, 4].round().long() + 1)
        prev_label_emb = self.model.prev_label_embedding(prev_label_idx)
        
        # Prev Token: Uses the fixed token ID from the specific sample (context)
        # We repeat it for the whole batch generated by SHAP
        fixed_token_ids = self.fixed_prev_token_id.repeat(batch_size)
        prev_token_emb = self.model.q_embedding(fixed_token_ids)
        
        # Sum them as per LACE architecture
        prev_feat = prev_token_emb + prev_label_emb

        # 4. Feature Fusion
        # Expand the fixed Transformer history to match batch size
        h_last_expanded = self.fixed_h_last.repeat(batch_size, 1)
        
        # Concatenate: [History, Lexical, Phonological, PrevInteraction]
        # Note: Order must match model_arch.py logic
        # E1_LACE usually does not use Context Features (ctx), so we skip them.
        fusion_vec = torch.cat([h_last_expanded, lex_feat, pho_feat, prev_feat], dim=1)

        # 5. Classification
        logits = self.model.classifier(fusion_vec).squeeze(-1)
        
        return logits.detach().cpu().numpy()

def main():
    # --- 1. Load Resources ---
    print(f"Loading resources for {MODEL_NAME}...")
    sizes = load_vocab_sizes()
    
    # Define the config used for E1_LACE (must match training)
    config = {
        "use_hist_pos_phoneme": True,
        "use_lexical": True,
        "use_phoneme": True,
        "use_context": False,
        "use_prev_interaction": True
    }
    
    # Initialize Model
    model = LACEModel(sizes, HPARAMS, config).to(DEVICE)
    
    # Load Weights
    if not os.path.exists(MODEL_PATH):
        raise FileNotFoundError(f"Model checkpoint not found: {MODEL_PATH}")
    
    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))
    model.eval()
    print("Model loaded successfully.")

    # --- 2. Prepare Data for SHAP ---
    print("Preparing test data...")
    test_set = UnifiedSAKTDataset("test")
    test_loader = DataLoader(test_set, batch_size=1, shuffle=True) # Batch=1 for easier context extraction

    collected_samples = []
    
    print(f"Extracting context for {N_SAMPLES} samples...")
    with torch.no_grad():
        for i, batch in enumerate(tqdm(test_loader, total=N_SAMPLES)):
            if i >= N_SAMPLES:
                break
            
            # Move batch to device
            for k in batch: batch[k] = batch[k].to(DEVICE)

            # A. Run the Transformer part to get the static history context (h_last)
            B, L = batch["q_seq"].size()
            
            # Replicate the forward pass logic for Sequence Encoding
            x = model.q_embedding(batch["q_seq"].long()) + \
                model.r_embedding(batch["r_seq"].long() + 1) + \
                model.position_embedding(torch.arange(L, device=DEVICE).unsqueeze(0))

            if config.get("use_hist_pos_phoneme"):
                x += model.pos_embedding(batch["pos_seq"].long())
                x += model.phoneme_embedding(batch["phoneme_seq"].long()).sum(dim=2)

            mask = torch.nn.Transformer.generate_square_subsequent_mask(L).to(DEVICE)
            h = model.transformer(x, mask)
            h_last = h[:, -1, :] # (1, d_model)

            # B. Extract the 5 Raw Features of Interest
            # Lexical: [Freq, Length]
            lex_raw = batch["next_lexical"].cpu().numpy() # (1, 2)
            # Phoneme: [Syllables, Challenging]
            pho_raw = batch["next_phoneme"].cpu().numpy() # (1, 2)
            # Prev Interaction: [TokenID, Label] -> We only want Label for SHAP
            prev_label = batch["next_prev_interaction"][:, 1].unsqueeze(-1).cpu().numpy() # (1, 1)
            prev_token_id_tensor = batch["next_prev_interaction"][:, 0] # Keep as tensor for embedding lookup later

            # Combine into a single vector (1, 5)
            x_raw_combined = np.concatenate([lex_raw, pho_raw, prev_label], axis=1)

            collected_samples.append({
                "x_raw": x_raw_combined,
                "h_last": h_last,
                "prev_token_id": prev_token_id_tensor
            })

    # --- 3. Run SHAP ---
    print("Calculating SHAP values (this may take time)...")
    
    # Prepare background dataset for KernelExplainer (using K-means summary for speed)
    all_x_raw = np.concatenate([s["x_raw"] for s in collected_samples], axis=0)
    background_summary = shap.kmeans(all_x_raw, 10) 

    final_shap_values = []
    
    # Iterate through each sample
    # We must instantiate a new Wrapper for each sample because 'h_last' changes per student
    for i in tqdm(range(len(collected_samples)), desc="Explaining"):
        sample = collected_samples[i]
        
        # Create wrapper with fixed context for this specific student/sequence
        wrapper = SHAPModelWrapper(model, sample["h_last"], sample["prev_token_id"])
        
        # Explain
        explainer = shap.KernelExplainer(wrapper.predict, background_summary, silent=True)
        shap_vals = explainer.shap_values(sample["x_raw"])
        
        # Handle SHAP output format (list vs array)
        if isinstance(shap_vals, list):
            final_shap_values.append(shap_vals[0]) # For regression/binary logits
        else:
            final_shap_values.append(shap_vals)

    # Aggregate results
    shap_values_matrix = np.concatenate(final_shap_values, axis=0)
    
    # --- 4. Plotting ---
    print("Generating Figure 4...")
    os.makedirs(os.path.dirname(OUTPUT_FIG_PATH), exist_ok=True)
    
    plt.figure(figsize=(10, 6), dpi=300)
    
    # Create the Summary Plot
    # sort=True automatically puts the most important feature at the top
    shap.summary_plot(
        shap_values_matrix, 
        all_x_raw, 
        feature_names=FEATURE_NAMES,
        show=False,
        cmap="coolwarm",
        alpha=0.8
    )
    
    # Customizing layout
    plt.title("Figure 4. Feature Contributions to Cognitive Load", fontsize=14, pad=20)
    plt.xlabel("SHAP value (Impact on model output)", fontsize=12)
    plt.tight_layout()
    
    # Save
    plt.savefig(OUTPUT_FIG_PATH)
    print(f"Figure saved to: {OUTPUT_FIG_PATH}")
    plt.show()

if __name__ == "__main__":
    main()